Tristan Rice
q7w9a, 25886145

Time Taken: 4h

# Question 1
## a)

set-index bits: log2(8) = 3
block-offset bits: log2(8) = 3
tag bits: 15 - (3+3) = 9


14 | 13 | 12 | 11 | 10 |  9 |  8 |  7 |  6 |  5 |  4 |  3 |  2 |  1 |  0
-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --
CT | CT | CT | CT | CT | CT | CT | CT | CT | CI | CI | CI | CO | CO | CO

## b)

```ruby
[0x0362, 0x7BFE, 0x3FC9, 0x0361].map do |n|
  [(n >> 3) & 0b111, n >> 6]
end.map do |n|
  n.map do |n|
    n.to_s 16
  end
end

# => [["4", "d"], ["7", "1ef"], ["1", "ff"], ["4", "d"]]
```

Operation | Address | Cache Set | Tag | Hit? | Value
--------- | ------- | --------- | --- | ---- | -----
Read      | 0x0362  | 4         | 00D | no   | unknown, since line isn't valid
Read      | 0x7BFE  | 7         | 1EF | no   | unknown, since line not in cache
Read      | 0x3FC9  | 1         | 0FF | no   | unknown, since line not in cache
Read      | 0x0361  | 4         | 00D | yes  | unknown, since line has been refreshed


# Question 2

## [3] a. An array with 64 rows and 64 columns, a direct-mapped cache, and the function sumA from the program cache-test provided with question 2.

Sum = 4326400
Miss rate =   0.2500

sumA reads all of the values in the array in the order they are laid out in
memory. Reading a[0][0] the first time loads that into the cache. Since the
blocks are 16 bytes long, and the values are 4 bytes long, that means a[1-3][0]
will hit the same block as a[0][0]. Thus you get a 0.25 miss rate since every 4
reads you read from a new block.

## [3] b. An array with 64 rows and 64 columns, a direct-mapped cache, and the function sumB from the program cache-test provided with question 2.

Sum = 4326400
Miss rate =   1.0000

sumB strides over the memory by 64 * 4 bytes, thus every memory lookup hits a
different block. Since 64 is a multiple of 128 you repeatedly hit the same cache
lines and only end up using 16 of 128. Thus, you miss every time since those
16 cache lines get completely overwritten every 16 reads.

## [3] c. An array with 64 rows and 64 columns, a direct-mapped cache, and the function sumC from the program cache-test provided with question 2.

Sum = 4326400
Miss rate =   0.5000

sumC strides over the memory in 8 byte x 8 byte chunks. Since it's hitting two
different columns, that means it has to be accessing a minimum of 2 lines. Since
the two reads on each line are right next to each other, they're most likely
hitting the same line. Thus you only have to load two lines for every iteration
of the inner for loop and miss 50% of the time.

## [3] d. An array with 68 rows and 68 columns, a direct-mapped cache, and the function sumB from the program cache-test provided with question 2.

Sum = 5503716
Miss rate =   0.2500

If you look at the access pattern, there is a repeating pattern every 68*4
memory accesses. The first 68 reads are misses, each with a different
cache_index and a block offset of 0. The next 68*3 reads are hits, with
cache_indexes matching the previous 68, and block offsets ranging from 0x4 to
0xc. Due to the stride of 68, every single row gets stored into it's own set and
thus despite the large stride, still is effectively cached.

## [3] e. An array with 48 rows and 48 columns, a direct-mapped cache, and the function sumB from the program cache-test provided with question 2.

Sum = 1382976
Miss rate =   0.2500

This is the same effect as 3.d. Since 48 isn't a multiple of 128, each row gets
its own cache line despite the large stride. There are 48 lines used, and every
4 reads a new block is read (16 bytes/4 bytes). Thus leading to a miss rate of
0.25.

## [3] f. An array with 48 rows and 48 columns, a two-way set-associative cache, and the function sumB from the program cache-test provided with question 2.

Sum = 1382976
Miss rate =   0.2500

This returns the same miss-rate as 3.e due to the fact that there's the same
number of total cache lines. Due to the two-way set-associative cache there are
now two lines per set, and half the number of sets. The total number of lines
remain constant. This means that the same amount of data can be cached and thus
has a similar miss rate.

